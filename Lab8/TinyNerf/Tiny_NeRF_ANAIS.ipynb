{"cells":[{"cell_type":"markdown","metadata":{"id":"lLDTVWKq7-ei"},"source":["##Tiny NeRF\n","This is a simplied version of the method presented in *NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis*\n","\n","[Project Website](http://www.matthewtancik.com/nerf)\n","\n","[arXiv Paper](https://arxiv.org/abs/2003.08934)\n","\n","[Full Code](github.com/bmild/nerf)\n","\n","Components not included in the notebook\n","*   5D input including view directions\n","*   Hierarchical Sampling\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZNXlxmEj0FC"},"outputs":[],"source":["import google.colab\n","\n","import os, sys\n","import tensorflow as tf\n","tf.compat.v1.enable_eager_execution()\n","\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mTxAwgrj4yn"},"outputs":[],"source":["if not os.path.exists('tiny_nerf_data.npz'):\n","    !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz"]},{"cell_type":"markdown","metadata":{"id":"t2dgdCDi-m3T"},"source":["# Load Input Images and Poses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jj1lof2ej0FI"},"outputs":[],"source":["data = np.load('tiny_nerf_data.npz')\n","images = data['images']\n","poses = data['poses']\n","focal = data['focal']\n","H, W = images.shape[1:3]\n","print(images.shape, poses.shape, focal)\n","\n","testimg, testpose = images[101], poses[101]\n","images = images[:100,...,:3]\n","poses = poses[:100]\n","\n","plt.imshow(testimg)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jxDt192E-v6i"},"source":["# Optimize NeRF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1avtwVoAQTu"},"outputs":[],"source":["\n","\n","def posenc(x):\n","  rets = [x]\n","  for i in range(L_embed):\n","    for fn in [tf.sin, tf.cos]:\n","      rets.append(fn(2.**i * x))\n","  return tf.concat(rets, -1)\n","\n","L_embed = 6\n","\n","### TODO < ------------------------------------------------- >\n","### Encoding Function to be used.\n","encode_fn = \n","\n","def init_model(D=8, W=256):\n","    relu = tf.keras.layers.ReLU()    \n","    dense = lambda W=W, act=relu : tf.keras.layers.Dense(W, activation=act)\n","\n","    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n","    outputs = inputs\n","    for i in range(D):\n","        outputs = dense()(outputs)\n","        if i%4==0 and i>0:\n","            outputs = tf.concat([outputs, inputs], -1)\n","    outputs = dense(4, act=None)(outputs)\n","    \n","    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","\n","def get_rays(H, W, focal, c2w):\n","    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n","    dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1)\n","    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n","    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n","    return rays_o, rays_d\n","\n","\n","\n","def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, rand=False):\n","\n","    def batchify(fn, chunk=1024*32):\n","        return lambda inputs : tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n","    \n","    # Compute 3D query points\n","    z_vals = tf.linspace(near, far, N_samples) \n","    if rand:\n","      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n","    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n","    \n","    # Run network\n","    pts_flat = tf.reshape(pts, [-1,3])\n","    pts_flat = encode_fn(pts_flat)\n","    raw = batchify(network_fn)(pts_flat)\n","    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n","    \n","    # Compute opacities and colors\n","    sigma_a = tf.nn.relu(raw[...,3])\n","    rgb = tf.math.sigmoid(raw[...,:3]) \n","    \n","    # Do volume rendering\n","    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n","    alpha = 1.-tf.exp(-sigma_a * dists)  \n","    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n","    \n","    rgb_map = tf.reduce_sum(weights[...,None] * rgb, -2) \n","    depth_map = tf.reduce_sum(weights * z_vals, -1) \n","    acc_map = tf.reduce_sum(weights, -1)\n","\n","    return rgb_map, depth_map, acc_map"]},{"cell_type":"markdown","metadata":{"id":"3TSAyVcKAiyI"},"source":["Here we optimize the model. We plot a rendered holdout view and its PSNR every 50 iterations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XurcHoCj0FQ"},"outputs":[],"source":["model = init_model()\n","optimizer = tf.keras.optimizers.Adam(5e-4)\n","# No of sample points per ray\n","N_samples = 64\n","### TODO < ------------------------------------------------- >\n","### No of Training iterations\n","N_iters = \n","psnrs = []\n","iternums = []\n","# Iteration interval at which image will be rendered\n","i_plot = 25\n","\n","import time\n","t = time.time()\n","for i in range(N_iters+1):\n","    \n","    img_i = np.random.randint(images.shape[0])\n","    target = images[img_i]\n","    pose = poses[img_i]\n","    rays_o, rays_d = get_rays(H, W, focal, pose)\n","    with tf.GradientTape() as tape:\n","        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples, rand=True)\n","        loss = tf.reduce_mean(tf.square(rgb - target))\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    \n","    if i%i_plot==0:\n","        print(i, (time.time() - t) / i_plot, 'secs per iter')\n","        t = time.time()\n","        \n","        # Render the holdout view for logging\n","        rays_o, rays_d = get_rays(H, W, focal, testpose)\n","        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n","        loss = tf.reduce_mean(tf.square(rgb - testimg))\n","        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n","\n","        psnrs.append(psnr.numpy())\n","        iternums.append(i)\n","        \n","        plt.figure(figsize=(10,4))\n","        plt.subplot(121)\n","        plt.imshow(rgb)\n","        plt.title(f'Iteration: {i}')\n","        plt.subplot(122)\n","        plt.plot(iternums, psnrs)\n","        plt.title('PSNR')\n","        plt.show()\n","\n","print('Done')"]},{"cell_type":"markdown","metadata":{"id":"bZLEFNox_UVK"},"source":["# Interactive Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L92jHDI7j0FT"},"outputs":[],"source":["%matplotlib inline\n","from ipywidgets import interactive, widgets\n","\n","\n","trans_t = lambda t : tf.convert_to_tensor([\n","    [1,0,0,0],\n","    [0,1,0,0],\n","    [0,0,1,t],\n","    [0,0,0,1],\n","], dtype=tf.float32)\n","\n","rot_phi = lambda phi : tf.convert_to_tensor([\n","    [1,0,0,0],\n","    [0,tf.cos(phi),-tf.sin(phi),0],\n","    [0,tf.sin(phi), tf.cos(phi),0],\n","    [0,0,0,1],\n","], dtype=tf.float32)\n","\n","rot_theta = lambda th : tf.convert_to_tensor([\n","    [tf.cos(th),0,-tf.sin(th),0],\n","    [0,1,0,0],\n","    [tf.sin(th),0, tf.cos(th),0],\n","    [0,0,0,1],\n","], dtype=tf.float32)\n","\n","\n","def pose_spherical(theta, phi, radius):\n","    c2w = trans_t(radius)\n","    c2w = rot_phi(phi/180.*np.pi) @ c2w\n","    c2w = rot_theta(theta/180.*np.pi) @ c2w\n","    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n","    return c2w\n","\n","\n","def f(**kwargs):\n","    c2w = pose_spherical(**kwargs)\n","    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n","    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n","    img = np.clip(rgb,0,1)\n","    \n","    plt.figure(2, figsize=(20,6))\n","    plt.imshow(img)\n","    plt.show()\n","    \n","\n","sldr = lambda v, mi, ma: widgets.FloatSlider(\n","    value=v,\n","    min=mi,\n","    max=ma,\n","    step=.01,\n",")\n","\n","names = [\n","    ['theta', [100., 0., 360]],\n","    ['phi', [-30., -90, 0]],\n","    ['radius', [4., 3., 5.]],\n","]\n","\n","interactive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\n","output = interactive_plot.children[-1]\n","output.layout.height = '350px'\n","interactive_plot"]},{"cell_type":"markdown","metadata":{"id":"PpKhAn2a__Iu"},"source":["# Render 360 Video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Sg4aV0cmVPs"},"outputs":[],"source":["frames = []\n","for th in tqdm(np.linspace(0., 360., 120, endpoint=False)):\n","    c2w = pose_spherical(th, -30., 4.)\n","    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n","    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n","    frames.append((255*np.clip(rgb,0,1)).astype(np.uint8))\n","\n","import imageio\n","f = 'video.mp4'\n","imageio.mimwrite(f, frames, fps=30, quality=7)\n","from IPython.display import HTML\n","from base64 import b64encode\n","mp4 = open('video.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=400 controls autoplay loop>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQ_ms-YMyFly"},"outputs":[],"source":["### TODO <------------------------------------------------->\n","# Render a video with depth images"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"venv-tf115","language":"python","name":"venv-tf115"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}