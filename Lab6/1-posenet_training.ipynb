{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1gPvfU9ny6XO3jUDbS-SZJ6Ko-VcgKQte","timestamp":1684760922539}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# I. Camera localization - PoseNet Training"],"metadata":{"id":"naMWEmmtmSLk"}},{"cell_type":"markdown","source":["## 1. Import depencies"],"metadata":{"id":"_LSSVe2Om6P0"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"XVQ4otCch-u2","executionInfo":{"status":"error","timestamp":1685326092893,"user_tz":-345,"elapsed":16,"user":{"displayName":"Vote For Change","userId":"13218201374606462002"}},"outputId":"818d4249-2b25-41e6-ce81-f3d1d7130284"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/ANAIS2023-PoseNet-Lab'\n","/content\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-6191e850e4a2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#drive.mount('/content/drive',force_mount=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'/content/drive/MyDrive/ANAIS2023-PoseNet-Lab'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpose_resnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pose_resnet'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from google.colab import drive\n","#drive.mount('/content/drive',force_mount=True)\n","%cd '/content/drive/MyDrive/ANAIS2023-PoseNet-Lab'\n","from pose_resnet import *\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import os\n","import numpy as np\n","import requests\n","!pip install tqdm\n","from tqdm import tqdm\n","from torchvision import models\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZthKO5W8U-5u","executionInfo":{"status":"ok","timestamp":1685277757282,"user_tz":-345,"elapsed":27678,"user":{"displayName":"Neha Verma","userId":"13532281502706188094"}},"outputId":"3ebbb03f-5777-4448-903a-5429751e64c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## 2. Download dataset"],"metadata":{"id":"mTZup15tnWdp"}},{"cell_type":"code","source":["# Specify the URL of the file to download\n","url = \"https://api.repository.cam.ac.uk/server/api/core/bitstreams/1cd2b04b-ada9-4841-8023-8207f1f3519b/content\"\n","\n","# Specify the destination file path\n","file_path = 'KingsCollege.zip'\n","\n","# Check if the file already exists\n","if not os.path.exists(file_path):\n","    # Download the file\n","    response = requests.get(url, stream=True)\n","    file_size = int(response.headers.get('Content-Length', 0))\n","    chunk_size = 1024\n","    num_bars = int(file_size / chunk_size)\n","    \n","    with open(file_path, 'wb') as f:\n","        for chunk in tqdm(response.iter_content(chunk_size=chunk_size), total=num_bars, unit='KB', desc='Downloading File', ascii=True, ncols=75):\n","            f.write(chunk)\n","    !unzip /content/KingsCollege.zip -d /content/\n","else:\n","    print(f\"File '{file_path}' already exists. Skipping download.\")\n","\n","# Extract the zip file\n","#!unzip /content/KingsCollege.zip -d /content/"],"metadata":{"id":"a3SyYf2bncfG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685277789424,"user_tz":-345,"elapsed":23,"user":{"displayName":"Neha Verma","userId":"13532281502706188094"}},"outputId":"5f53d7c7-67b0-4651-ec3c-80ee8b2eee86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File 'KingsCollege.zip' already exists. Skipping download.\n"]}]},{"cell_type":"markdown","source":["## 3. DataLoader"],"metadata":{"id":"iZr0qnr9ng40"}},{"cell_type":"code","source":["class PoseDataset(Dataset):\n","    def __init__(self, root_dir, file_path, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.data = []\n","        with open(file_path, 'r') as f:\n","            lines = f.readlines()[3:]  # skip the header\n","            for line in lines:\n","                items = line.split()\n","                image_path = os.path.join(root_dir, items[0])\n","                image = Image.open(image_path)\n","                if self.transform:\n","                    image = self.transform(image)\n","                pose = list(map(float, items[1:]))\n","                self.data.append((image, torch.tensor(pose)))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image, pose = self.data[idx]\n","        return image, pose\n","\n","# Define the transformations\n","transform = transforms.Compose([\n","    transforms.Resize(260),\n","    transforms.CenterCrop(250),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])"],"metadata":{"id":"umxNJYCQny5A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Define loss function"],"metadata":{"id":"eM-tdS15oId7"}},{"cell_type":"code","source":["# Define the loss function\n","class BalancedMSELoss(nn.Module):\n","    def __init__(self):\n","        super(BalancedMSELoss, self).__init__()\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, pred, target):\n","        trans_pred, rot_pred = pred[:, :3], pred[:, 3:]\n","        trans_target, rot_target = target[:, :3], target[:, 3:]\n","        trans_loss = self.mse(trans_pred, trans_target)\n","        rot_loss = self.mse(rot_pred, rot_target)\n","        loss = trans_loss + 500*rot_loss\n","        return loss"],"metadata":{"id":"MqsGddbVoYYb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Training"],"metadata":{"id":"2IUiLATVobeL"}},{"cell_type":"code","source":["def train(model, train_loader, device, criterion, optimizer):\n","    model.train()\n","\n","    total_loss = 0\n","    total_images = 0\n","    for i, (images, poses) in enumerate(train_loader):\n","        images = images.to(device)\n","        poses = poses.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, poses)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * images.size(0)\n","        total_images += images.size(0)\n","\n","    # Return the average loss for this epoch\n","    avg_loss = total_loss / total_images\n","    print(f'Average Loss this epoch: {avg_loss}')\n","    return avg_loss"],"metadata":{"id":"sU9LzQVpoiGY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Testing"],"metadata":{"id":"TZFatpSSokOj"}},{"cell_type":"code","source":["def test(model, test_loader, device, criterion):\n","    model.eval()\n","    losses = []\n","    with torch.no_grad():\n","        for images, poses in test_loader:\n","            images = images.to(device)\n","            poses = poses.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, poses)\n","            losses.append(loss.item() * images.size(0))\n","\n","    # Return the average loss and errors for this test run\n","    avg_loss = sum(losses) / len(losses)\n","\n","    print(f'Average MSE on the test set: {avg_loss}')\n","    return avg_loss"],"metadata":{"id":"HSg_rhz9otsL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7.Main"],"metadata":{"id":"JWAI8rY9pCjC"}},{"cell_type":"code","source":["def main():\n","    # Hyperparameters\n","    num_epochs = 1000\n","    test_interval = 5  # Run test every 'test_interval' epochs\n","    learning_rate = 0.005\n","\n","    # Device configuration\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Create the model\n","    model = PoseResNet().to(device)\n","\n","    # Define the loss and the optimizer\n","    criterion = BalancedMSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Load the data\n","    root_dir = \"./KingsCollege/\"\n","    train_dataset = PoseDataset(root_dir, root_dir + \"dataset_train.txt\", transform=transform)\n","    test_dataset = PoseDataset(root_dir, root_dir + \"dataset_test.txt\", transform=transform)\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=8)\n","    test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers=8)\n","\n","    # Create the linear learning rate scheduler\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n","\n","    # Lists to keep track of loss\n","    train_losses = []\n","    test_losses = []\n","\n","    for epoch in range(num_epochs):\n","        train_loss = train(model, train_loader, device, criterion, optimizer)\n","        scheduler.step()  \n","        train_losses.append(train_loss)\n","\n","        # Test the model every 'test_interval' epochs\n","        if (epoch % test_interval == 0 or epoch == num_epochs - 1) and epoch != 0:\n","            test_loss = test(model, test_loader, device, criterion)\n","            test_losses.append(test_loss)\n","\n","            # Plot the losses\n","            fig2 = plt.figure()  # Create a new figure for loss plot\n","            plt.plot(train_losses, label='Train loss')\n","            plt.plot(range(test_interval, len(train_losses)+1, test_interval), test_losses, label='Test loss')\n","            plt.title('Train and Test Loss')\n","            plt.xlabel('Epochs')\n","            plt.ylabel('Loss')\n","            plt.legend()\n","            plt.show()  # Show the loss plot\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"5RgcXEA3pMxa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"783e0e88-e666-4582-cf10-3dbebb74212e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 81.2MB/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5tI41nujV2i1"},"execution_count":null,"outputs":[]}]}